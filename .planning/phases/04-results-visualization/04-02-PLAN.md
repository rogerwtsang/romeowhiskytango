---
phase: 04-results-visualization
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/simulation/batch.py
  - src/gui/dashboard/results_panel.py
  - src/gui/dashboard/main_dashboard.py
autonomous: true

must_haves:
  truths:
    - "User sees win probability with confidence interval in results"
    - "User sees LOB per game summary"
    - "User sees RISP conversion rate"
    - "Summary metrics are prominent in results panel"
  artifacts:
    - path: "src/simulation/batch.py"
      provides: "Extended summary statistics"
      contains: ["win_probability", "lob_per_game", "risp_conversion"]
    - path: "src/gui/dashboard/results_panel.py"
      provides: "Enhanced summary display"
      min_lines: 350
  key_links:
    - from: "src/gui/dashboard/results_panel.py"
      to: "batch.py summary output"
      via: "result_data dict keys"
      pattern: "result_data\\.get\\(['\"]win_"
---

<objective>
Add win probability, LOB (left on base), and scoring efficiency metrics to the simulation output and display them prominently in ResultsPanel.

Purpose: Deliver core "visual clarity" value by showing actionable metrics. Win probability answers "how good is this lineup?" while LOB/RISP show efficiency gaps.
Output: Enhanced summary statistics in simulation output, new metrics in ResultsPanel
</objective>

<execution_context>
@/home/roger/.claude/get-shit-done/workflows/execute-plan.md
@/home/roger/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/04-results-visualization/04-CONTEXT.md
@src/simulation/batch.py
@src/gui/dashboard/results_panel.py
@src/gui/dashboard/main_dashboard.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add win probability and efficiency metrics to batch simulation</name>
  <files>src/simulation/batch.py</files>
  <action>
1. In `run_simulations()` or the summary calculation function, add new metrics:

   **Win Probability Calculation:**
   - Count seasons where runs scored >= threshold (league average ~4.5 runs/game * 162 games = 729 runs/season)
   - Use a simple threshold model: P(win) = (seasons above league avg) / total_seasons
   - Calculate 95% CI using Wilson score interval (more accurate for proportions)
   - Store as: win_probability = {mean, ci_lower, ci_upper}

   **LOB Per Game:**
   - If LOB data tracked in raw_data, compute: lob_per_game = total_lob / (162 * n_simulations)
   - If LOB not currently tracked, add it to game state tracking OR use a proxy estimate
   - Store as: lob_per_game = {mean, std}

   **RISP Conversion Rate:**
   - If RISP data available: conversion_rate = runs_scored_with_risp / risp_opportunities
   - If not available, estimate from hits with runners in scoring position
   - Store as: risp_conversion = {rate, ci_lower, ci_upper}

2. Add these to the summary dict returned by run_simulations():
   ```python
   summary = {
       'runs': {...},  # existing
       'runs_per_game': {...},  # existing
       'win_probability': {'mean': float, 'ci_lower': float, 'ci_upper': float},
       'lob_per_game': {'mean': float, 'std': float},
       'risp_conversion': {'rate': float, 'ci_lower': float, 'ci_upper': float},
   }
   ```

NOTE: If LOB/RISP data is not currently tracked in the simulation, add placeholder values with a TODO comment for future enhancement. The display layer should handle missing/None values gracefully.
  </action>
  <verify>
```bash
# Type check
mypy src/simulation/batch.py --ignore-missing-imports

# Verify function signature unchanged (existing callers still work)
python -c "from src.simulation.batch import run_simulations; import inspect; sig = inspect.signature(run_simulations); print('Signature OK:', sig)"
```
  </verify>
  <done>
- batch.py returns win_probability with CI
- batch.py returns lob_per_game (or placeholder)
- batch.py returns risp_conversion (or placeholder)
- Existing return structure maintained (backward compatible)
  </done>
</task>

<task type="auto">
  <name>Task 2: Display enhanced metrics in ResultsPanel summary</name>
  <files>src/gui/dashboard/results_panel.py, src/gui/dashboard/main_dashboard.py</files>
  <action>
1. Modify `_create_summary_section()` in ResultsPanel to add new metrics:

   Add after existing rows (CI, Iterations):
   - Row 4: "Win Probability:" with label (e.g., "87% [82-92%]")
   - Row 5: "LOB per Game:" with label (e.g., "6.2 +/- 1.1")
   - Row 6: "RISP Conversion:" with label (e.g., "24.5% [23-26%]")

   Create instance variables:
   - self.win_prob_label
   - self.lob_label
   - self.risp_label

2. Modify `display_results()` to populate new labels:
   ```python
   # Win probability
   win_prob = result_data.get('win_probability', {})
   if win_prob:
       wp_mean = win_prob.get('mean', 0) * 100  # Convert to percentage
       wp_lower = win_prob.get('ci_lower', 0) * 100
       wp_upper = win_prob.get('ci_upper', 0) * 100
       self.win_prob_label.config(text=f"{wp_mean:.0f}% [{wp_lower:.0f}-{wp_upper:.0f}%]")
   else:
       self.win_prob_label.config(text="--")

   # Similar for LOB and RISP
   ```

3. Modify `clear_results()` to reset new labels to "--"

4. Update `_normalize_results()` in main_dashboard.py to pass through new metrics:
   - Extract win_probability, lob_per_game, risp_conversion from results summary
   - Add to normalized dict

IMPORTANT: Handle missing data gracefully. If metrics are None or missing, display "--" rather than crashing.
  </action>
  <verify>
```bash
# Type check both files
mypy src/gui/dashboard/results_panel.py src/gui/dashboard/main_dashboard.py --ignore-missing-imports

# Verify imports still work
python -c "from src.gui.dashboard.results_panel import ResultsPanel; from src.gui.dashboard.main_dashboard import MainDashboard; print('Imports OK')"
```
  </verify>
  <done>
- ResultsPanel shows win probability with CI
- ResultsPanel shows LOB per game
- ResultsPanel shows RISP conversion rate
- Missing data displays as "--" (graceful degradation)
- No type errors
  </done>
</task>

</tasks>

<verification>
```bash
# Type check all modified files
mypy src/simulation/batch.py src/gui/dashboard/results_panel.py src/gui/dashboard/main_dashboard.py --ignore-missing-imports

# Verify batch still runs (quick test with minimal iterations)
python -c "
from src.models.player import Player
from src.simulation.batch import run_simulations

# Create minimal test lineup
test_players = [
    Player(name=f'Test{i}', ba=0.250, obp=0.320, slg=0.400, k_pct=0.20)
    for i in range(9)
]

# Run minimal simulation
results = run_simulations(test_players, n_simulations=2, games_per_season=10)
print('Summary keys:', results['summary'].keys())
print('Has win_probability:', 'win_probability' in results['summary'])
"
```
</verification>

<success_criteria>
- Win probability displayed as percentage with CI
- LOB per game displayed (or placeholder if data unavailable)
- RISP conversion displayed (or placeholder if data unavailable)
- Summary section remains clean and readable
- No regressions in existing metrics display
</success_criteria>

<output>
After completion, create `.planning/phases/04-results-visualization/04-02-SUMMARY.md`
</output>
