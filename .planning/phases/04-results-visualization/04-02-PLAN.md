---
phase: 04-results-visualization
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/simulation/batch.py
  - src/gui/dashboard/results_panel.py
  - src/gui/dashboard/main_dashboard.py
autonomous: true

must_haves:
  truths:
    - "User sees win probability with confidence interval in results"
    - "User sees LOB per game summary (or '--' if unavailable)"
    - "User sees RISP conversion rate (or '--' if unavailable)"
    - "Summary metrics are prominent in results panel"
  artifacts:
    - path: "src/simulation/batch.py"
      provides: "Extended summary statistics"
      contains: ["win_probability", "lob_per_game"]
    - path: "src/gui/dashboard/results_panel.py"
      provides: "Enhanced summary display"
      min_lines: 350
  key_links:
    - from: "src/gui/dashboard/results_panel.py"
      to: "batch.py summary output"
      via: "result_data dict keys"
      pattern: "result_data\\.get\\(['\"]win_"
    - from: "src/simulation/batch.py"
      to: "src/simulation/season.py"
      via: "LOB aggregation from season results"
      pattern: "season_lob.*append"
---

<objective>
Add win probability, LOB (left on base), and scoring efficiency metrics to the simulation output and display them prominently in ResultsPanel.

Purpose: Deliver core "visual clarity" value by showing actionable metrics. Win probability answers "how good is this lineup?" while LOB/RISP show efficiency gaps.
Output: Enhanced summary statistics in simulation output, new metrics in ResultsPanel
</objective>

<execution_context>
@/home/roger/.claude/get-shit-done/workflows/execute-plan.md
@/home/roger/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/04-results-visualization/04-CONTEXT.md
@src/simulation/batch.py
@src/simulation/season.py
@src/gui/dashboard/results_panel.py
@src/gui/dashboard/main_dashboard.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add win probability and efficiency metrics to batch simulation</name>
  <files>src/simulation/batch.py</files>
  <action>
1. Add LOB tracking array collection (similar to existing patterns at lines 44-49):
   - Add `season_lob = []` after existing season tracking lists (after line 49)
   - In the simulation loop, append: `season_lob.append(result.get('total_lob', 0))`
   - Convert to numpy array: `season_lob_arr = np.array(season_lob)` (after line 83)

2. Calculate new metrics after existing statistics (after line 149):

   **Win Probability Calculation:**
   ```python
   # Win probability: proportion of seasons above league average runs
   # League average ~4.5 runs/game * 162 games = 729 runs/season
   league_avg_runs = 4.5 * n_games
   wins_above_avg = np.sum(season_runs_arr >= league_avg_runs)
   win_prob_mean = wins_above_avg / n_iterations

   # Wilson score interval for 95% CI (more accurate for proportions)
   from scipy.stats import norm
   z = norm.ppf(0.975)  # 1.96 for 95% CI
   n = n_iterations
   p_hat = win_prob_mean
   denominator = 1 + z**2 / n
   center = (p_hat + z**2 / (2*n)) / denominator
   spread = z * np.sqrt((p_hat * (1 - p_hat) + z**2 / (4*n)) / n) / denominator
   win_prob_lower = max(0, center - spread)
   win_prob_upper = min(1, center + spread)
   ```

   **LOB Per Game:**
   ```python
   # LOB per game from tracked season totals
   lob_per_game_mean = float(np.mean(season_lob_arr) / n_games)
   lob_per_game_std = float(np.std(season_lob_arr) / n_games)
   ```

   **RISP Conversion Rate:**
   ```python
   # RISP data is NOT currently tracked in game engine
   # Placeholder with None values - GUI handles gracefully
   risp_conversion = None  # TODO: Add RISP tracking to game engine in future phase
   ```

3. Add to summary dict (inside the existing summary = {...} block):
   ```python
   'win_probability': {
       'mean': float(win_prob_mean),
       'ci_lower': float(win_prob_lower),
       'ci_upper': float(win_prob_upper)
   },
   'lob_per_game': {
       'mean': lob_per_game_mean,
       'std': lob_per_game_std
   },
   'risp_conversion': risp_conversion,  # None until tracking added
   ```

4. Add `season_lob` to raw_data dict for potential future analysis

5. Add scipy import at top of file: `from scipy.stats import norm`

AVOID: Attempting to calculate RISP conversion - the game engine does not track this data. Use None placeholder.
  </action>
  <verify>
```bash
# Type check
mypy src/simulation/batch.py --ignore-missing-imports

# Verify function signature unchanged (existing callers still work)
python -c "from src.simulation.batch import run_simulations; import inspect; sig = inspect.signature(run_simulations); print('Signature OK:', sig)"

# Verify new fields exist in output
python -c "
from src.models.player import Player
from src.simulation.batch import run_simulations

# Minimal test
players = [Player(name=f'Test{i}', ba=0.250, obp=0.320, slg=0.400, k_pct=0.20) for i in range(9)]
results = run_simulations(players, n_iterations=10, n_games=10)
summary = results['summary']
print('win_probability:', 'win_probability' in summary)
print('lob_per_game:', 'lob_per_game' in summary)
print('risp_conversion:', 'risp_conversion' in summary)
print('LOB mean:', summary.get('lob_per_game', {}).get('mean', 'MISSING'))
"
```
  </verify>
  <done>
- batch.py returns win_probability with Wilson score CI
- batch.py returns lob_per_game calculated from season.py's total_lob
- batch.py returns risp_conversion as None placeholder
- Existing return structure maintained (backward compatible)
- scipy.stats.norm imported for Wilson score calculation
  </done>
</task>

<task type="auto">
  <name>Task 2: Display enhanced metrics in ResultsPanel summary</name>
  <files>src/gui/dashboard/results_panel.py, src/gui/dashboard/main_dashboard.py</files>
  <action>
1. Modify `_create_summary_section()` in ResultsPanel to add new metrics:

   Add after existing rows (CI, Iterations):
   - Row 4: "Win Probability:" with label (e.g., "87% [82-92%]")
   - Row 5: "LOB per Game:" with label (e.g., "6.2 +/- 1.1")
   - Row 6: "RISP Conversion:" with label (e.g., "24.5% [23-26%]" or "--" if unavailable)

   Create instance variables:
   - self.win_prob_label
   - self.lob_label
   - self.risp_label

2. Modify `display_results()` to populate new labels:
   ```python
   # Win probability
   win_prob = result_data.get('win_probability', {})
   if win_prob:
       wp_mean = win_prob.get('mean', 0) * 100  # Convert to percentage
       wp_lower = win_prob.get('ci_lower', 0) * 100
       wp_upper = win_prob.get('ci_upper', 0) * 100
       self.win_prob_label.config(text=f"{wp_mean:.0f}% [{wp_lower:.0f}-{wp_upper:.0f}%]")
   else:
       self.win_prob_label.config(text="--")

   # LOB per game
   lob = result_data.get('lob_per_game', {})
   if lob:
       lob_mean = lob.get('mean', 0)
       lob_std = lob.get('std', 0)
       self.lob_label.config(text=f"{lob_mean:.1f} +/- {lob_std:.1f}")
   else:
       self.lob_label.config(text="--")

   # RISP conversion (placeholder handling)
   risp = result_data.get('risp_conversion')
   if risp:
       rate = risp.get('rate', 0) * 100
       self.risp_label.config(text=f"{rate:.1f}%")
   else:
       self.risp_label.config(text="--")  # Graceful degradation
   ```

3. Modify `clear_results()` to reset new labels to "--"

4. Update `_normalize_results()` in main_dashboard.py to pass through new metrics:
   - Extract win_probability, lob_per_game, risp_conversion from results summary
   - Add to normalized dict (preserving None for risp_conversion)

IMPORTANT: Handle missing data gracefully. If metrics are None or missing, display "--" rather than crashing. This supports graceful degradation for RISP which is not yet tracked.
  </action>
  <verify>
```bash
# Type check both files
mypy src/gui/dashboard/results_panel.py src/gui/dashboard/main_dashboard.py --ignore-missing-imports

# Verify imports still work
python -c "from src.gui.dashboard.results_panel import ResultsPanel; from src.gui.dashboard.main_dashboard import MainDashboard; print('Imports OK')"
```
  </verify>
  <done>
- ResultsPanel shows win probability with CI
- ResultsPanel shows LOB per game with std dev
- ResultsPanel shows RISP conversion rate (or "--" when unavailable)
- Missing data displays as "--" (graceful degradation)
- No type errors
  </done>
</task>

</tasks>

<verification>
```bash
# Type check all modified files
mypy src/simulation/batch.py src/gui/dashboard/results_panel.py src/gui/dashboard/main_dashboard.py --ignore-missing-imports

# Verify batch still runs and returns new fields
python -c "
from src.models.player import Player
from src.simulation.batch import run_simulations

# Create minimal test lineup
test_players = [
    Player(name=f'Test{i}', ba=0.250, obp=0.320, slg=0.400, k_pct=0.20)
    for i in range(9)
]

# Run minimal simulation
results = run_simulations(test_players, n_iterations=10, n_games=10)
summary = results['summary']
print('Summary keys:', list(summary.keys()))
print('Has win_probability:', 'win_probability' in summary)
print('Has lob_per_game:', 'lob_per_game' in summary)
print('LOB mean value:', summary.get('lob_per_game', {}).get('mean'))
"
```
</verification>

<success_criteria>
- Win probability displayed as percentage with CI
- LOB per game displayed with mean and std
- RISP conversion displays "--" (placeholder until tracking added)
- Summary section remains clean and readable
- No regressions in existing metrics display
</success_criteria>

<output>
After completion, create `.planning/phases/04-results-visualization/04-02-SUMMARY.md`
</output>
